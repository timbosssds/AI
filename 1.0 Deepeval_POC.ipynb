{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 01.11.24\n",
    "# Purpose: Testing Deepeval\n",
    "# Theme: Eval\n",
    "# Status: POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fW1c_2pIHpN",
    "outputId": "2f9a87a1-150f-493a-bcee-6be52ae69d6a"
   },
   "outputs": [],
   "source": [
    "# Needed for package installs\n",
    "!pip install instructor\n",
    "!pip install deepeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NujzfJ6W9VR"
   },
   "source": [
    "# -- This is round one: works as a baseline --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pW0P6psL57n"
   },
   "outputs": [],
   "source": [
    "# NEEDED - to set up model\n",
    "# (Part 1)\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "# Set your API key here\n",
    "\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# Define a schema for the response\n",
    "class JokeResponseSchema(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\"\n",
    "\n",
    "# Create an instance of the schema\n",
    "schema_instance = JokeResponseSchema\n",
    "\n",
    "custom_llm = CustomGeminiFlash()\n",
    "# Pass the schema instance when calling generate\n",
    "#print(custom_llm.generate(\"Write me a joke\", schema=schema_instance))\n",
    "print(custom_llm.generate(\"What is the capital of France?\", schema=schema_instance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkpT0FotQU2u"
   },
   "outputs": [],
   "source": [
    "# NOT Needed (just testing above)\n",
    "print(custom_llm.generate(\"What is the capital of Spain?\", schema=schema_instance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sEvpam1N0sq"
   },
   "outputs": [],
   "source": [
    "# Eval One (from GPT I think). Returns reason (that looks valid), but only ever returns 'None' for Score\n",
    "# -- Keep, but this is a baseline not final\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Define your input and actual output\n",
    "#input = \"What is the capital of France?\"\n",
    "#actual_output = \"The capital of France is Paris.\"\n",
    "#actual_output = \"The capital of France is Spain.\"\n",
    "#actual_output = (custom_llm.generate(\"What is the capital of Spain?\", schema=schema_instance))\n",
    "\n",
    "input = \"Who is the name of the english premier league team, based out of london whose logo is a cannon?\"\n",
    "#actual_output = \"Manchester united.\"\n",
    "actual_output = \"Arsenal.\"\n",
    "#actual_output = (custom_llm.generate(\"Who is the name of the english premier league team, based out of london whose logo is a cannon?\", schema=schema_instance))\n",
    "\n",
    "# Create a test case\n",
    "test_case = LLMTestCase(input=input, actual_output=actual_output)\n",
    "\n",
    "# Initialize the Answer Relevancy Metric\n",
    "metric = AnswerRelevancyMetric(threshold=0.0, model=custom_llm, include_reason=True)\n",
    "#metric = AnswerRelevancyMetric(threshold=0.0, model=custom_llm, include_reason=True, verbose_mode=True)\n",
    "#metric = AnswerRelevancyMetric(threshold=0.0, model=custom_llm, include_reason=True, strict_mode=False)\n",
    "\n",
    "# Measure the relevancy score\n",
    "score = metric.measure(test_case)\n",
    "\n",
    "# Print the score and reason (if included)\n",
    "print(\"Score:\", score)\n",
    "print(\"Reason:\", metric.reason)  # This will provide insights into the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HwPvJ7uN0n3"
   },
   "outputs": [],
   "source": [
    "# Eval Two (from DeepEval*). Returns everything, when i use their suggested Q&A, but only partial when i use my LLM output. There must still be\n",
    "# formatting issues from my LLM output, but this is good progress. Fix the LLM output format and golden\n",
    "# -- Keep, but this is a baseline not final\n",
    "# * https://docs.confident-ai.com/docs/metrics-answer-relevancy (lots more metrics here to dig into once this working fully)\n",
    "# (Part 2)\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "#actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "actual_output = (custom_llm.generate(\"Who is the name of the english premier league team, based out of london whose logo is a cannon?\", schema=schema_instance))\n",
    "#actual_output = \"Dallas Cowboys\"\n",
    "#actual_output = \"American football club\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.25,\n",
    "    model=custom_llm,\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    #input=\"What if these shoes don't fit?\",\n",
    "    input = \"Who is the name of the english premier league team, based out of london whose logo is a cannon?\",\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SsfjQlZDN0iP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9fhnk98XGZP"
   },
   "source": [
    "# -- This is round two: limited testing, think works (ie. uses llm inference * scores that.) --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ekazo-NCDinG"
   },
   "outputs": [],
   "source": [
    "# --- Ground truth calls from llm ---\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key (replace with your actual key)\n",
    "\n",
    "\n",
    "# Define your question as a variable\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Start chat session\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "#instructional_prompt = \"**You are a helpful agent, who answers questions and formats the reponse in a professional way.** \"\n",
    "#prompt = instructional_prompt + text_to_spell\n",
    "prompt = \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\"\n",
    "response = chat_session.send_message(prompt)\n",
    "print(response.text)\n",
    "# --- Ground truth ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g1CHNH32P0_"
   },
   "outputs": [],
   "source": [
    "# # WIP: recreate part 1\n",
    "# # retired and replaced with below\n",
    "# from pydantic import BaseModel\n",
    "# import google.generativeai as genai\n",
    "# import instructor\n",
    "\n",
    "# from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# # Define your schema class here\n",
    "# class YourSchemaClass(BaseModel):\n",
    "#     response: str  # Define the fields you expect in the response\n",
    "\n",
    "# class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "#     def __init__(self, api_key: str):\n",
    "#         self.api_key = api_key\n",
    "#         genai.configure(api_key=self.api_key)  # Configure the API key\n",
    "#         self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "#     def load_model(self):\n",
    "#         return self.model\n",
    "\n",
    "#     def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "#         client = self.load_model()\n",
    "#         instructor_client = instructor.from_gemini(\n",
    "#             client=client,\n",
    "#             mode=instructor.Mode.GEMINI_JSON,\n",
    "#         )\n",
    "#         resp = instructor_client.messages.create(\n",
    "#             messages=[\n",
    "#                 {\n",
    "#                     \"role\": \"user\",\n",
    "#                     \"content\": prompt,\n",
    "#                 }\n",
    "#             ],\n",
    "#             response_model=schema,\n",
    "#             #temperature=0.2,  # Add the temperature parameter here\n",
    "\n",
    "#         )\n",
    "#         return resp\n",
    "\n",
    "#     async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "#         return self.generate(prompt, schema)\n",
    "\n",
    "#     def get_model_name(self):\n",
    "#         return \"Gemini 1.5 Flash\"\n",
    "\n",
    "# # Now you can use your API key to create an instance\n",
    "\n",
    "# custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "\n",
    "# # Call the generate method\n",
    "# response = custom_llm.generate(\"Has Chelase won the FA Cup the most any team has won the FA Cup?\", schema=YourSchemaClass)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L839Gqf7FWNq"
   },
   "outputs": [],
   "source": [
    "# WIP: recreate part 1\n",
    "# Replaces previous version. This one adds temperature\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# Define your schema class here\n",
    "class YourSchemaClass(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=self.api_key)  # Configure the API key\n",
    "\n",
    "        # Define generation configuration with temperature\n",
    "        generation_config = {\n",
    "            \"temperature\": 0,  # Set your desired temperature here\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"top_p\": 0.95,\n",
    "             \"top_k\": 64,\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"models/gemini-1.5-flash\",\n",
    "            generation_config=generation_config  # Pass the config here\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\"\n",
    "\n",
    "# Now you can use your API key to create an instance\n",
    "#api_key =   # Replace with your actual API key\n",
    "custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "\n",
    "# Call the generate method\n",
    "#response = custom_llm.generate(\"Have aliens been detected on earth and do they walk amongst us?\", schema=YourSchemaClass)\n",
    "response = custom_llm.generate(\"Has Chelsea won the FA Cup the most any team has won the FA Cup?\", schema=YourSchemaClass)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qj0dmSOB2Puo"
   },
   "outputs": [],
   "source": [
    "# As per above, but reconfiguring so question is asked once only\n",
    "# Eval Two (from DeepEval*)\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "question = \"Has Chelase won the FA Cup the most any team has won the FA Cup?\"\n",
    "\n",
    "# # Generate output from the LLM using the class as schema\n",
    "# actual_output_instance = custom_llm.generate(\n",
    "#     #\"Who is the name of the English Premier League team, based out of London whose logo is a cannon?\",\n",
    "#     #\"How many times has Chelase won the FA Cup and is that the most any team has won the FA Cup?\",\n",
    "#     #\"Has Chelase won the FA Cup the most any team has won the FA Cup?\",\n",
    "#     #question,\n",
    "#     \"No, Chelsea is not the team that has won the FA Cup the most. **Arsenal** holds the record for most FA Cup wins with **14 titles**. \\\n",
    "#     Chelsea has won the FA Cup **8 times**, which places them in 5th place on the list of most FA Cup winners.\",\n",
    "#     schema=YourSchemaClass  # Pass the class, not an instance\n",
    "# )\n",
    "\n",
    "# # Extract the response from the actual_output_instance\n",
    "# actual_output = actual_output_instance.response  # Get the response string\n",
    "\n",
    "# actual_output = \"No, Chelsea is not the team that has won the FA Cup the most. **Arsenal** holds the record for most FA Cup wins with **14 titles**. \\\n",
    "#                  Chelsea has won the FA Cup **8 times**, which places them in 5th place on the list of most FA Cup winners.\"\n",
    "#actual_output = \"Yes, Chelsea have won the FA Cup the most times. If fact, the have won it 70 times, which is 75 times more than any other team. The next closest team is Sydney FC\"\n",
    "actual_output = \"While Arsenal have won the cup an impressive 14 times, yes it is indeed Chelsea who have won the FA Cup the most times\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.75,\n",
    "    model=custom_llm,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    #input=\"Who is the name of the English Premier League team, based out of London whose logo is a cannon?\",\n",
    "    #input=\"Has Chelase won the FA Cup the most any team has won the FA Cup?\",\n",
    "    input=question,\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "# Measure and print the score and reason\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# Optionally evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZdb_gCLgDWV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmjOOpAKg3yJ"
   },
   "source": [
    "# -- This is round three: Merges Part 1 (a & b) + Part 2 + 2a --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFN2-4exgC2M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 694,
     "referenced_widgets": [
      "4bc612e810b14f5aa121d3b4bcee37a6",
      "b76a87649aa2448b8d96e7872d5b9f47",
      "0ce3fbe84f7843c0986334449e09026b",
      "a596db509621467db7b2ec981aff3810",
      "73fad46582594510ae8a92ec6a35d409",
      "bde9cd1bd86f4d7f947b090c1a94cf7b"
     ]
    },
    "id": "ntoKX2ZOFVuM",
    "outputId": "d8f45593-a7b8-4d2f-b53f-26c96f7a8208"
   },
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# ----- This is the one ---\n",
    "# -------------------------\n",
    "# Following some learnings from discord, it looks like the eval is to eval Q&A NOT generate the A, which is odd. So this will combine part 1 (to generate the A)\n",
    "# and part 2 (the eval - using the Q&A from part 1)\n",
    "\n",
    "# ----------------------------------------------\n",
    "context = \"On a sun-drenched Saturday morning, Elara, a spirited young archaeologist with a penchant \\\n",
    "          for adventure, set off with her best friend Leo to explore the ancient ruins of Eldoria, hidden \\\n",
    "          deep within the Whispering Woods. Armed with a tattered map she had discovered in her grandmother (Jenny) \\\n",
    "          attic, they trekked through lush greenery, their excitement palpable. After a few hours of hiking, they \\\n",
    "          stumbled upon a crumbling stone archway overgrown with ivy, marking the entrance to the fabled Temple of Echoes. \\\n",
    "          With only $50 in their pockets, they decided to splurge on a couple of sandwiches and bottled water from a \\\n",
    "          nearby vendor before diving into their exploration. Given they didn't have much money, they only sppent $15 as they wanted \\\n",
    "          to keep some money in case of any emergencies. Inside the temple, they uncovered intricate carvings \\\n",
    "          and mysterious symbols that hinted at a long-lost treasure. As they navigated through dark corridors and \\\n",
    "          avoided booby traps, Elara's heart raced with the thrill of discovery. By sunset, they emerged victorious, \\\n",
    "          not with gold but with priceless knowledge and a newfound bond, proving that sometimes the greatest treasures \\\n",
    "          are the adventures shared along the way.\"\n",
    "\n",
    "context2 = \"Eva Longoria has lifted the lid on the fortune she’s making from a blockbuster film she quietly helped finance a decade ago. \\\n",
    "            Last week, it was revealed the Desperate Housewives actress, 49, had secretly contributed $US6 million to get the first John Wick movie over the line.\\\n",
    "            The franchise has since gone on to produce four films, all starring US actor Keanu Reeves as the titular hit man, with a combined gross of more than $US1 billion.\\\n",
    "            Reminiscing on the action franchise’s 10th anniversary, directors Chad Stahelski and David Leitch told Business Insider they were almost forced to shut \\\n",
    "            down production on the 2014 movie at the 11th hour due to a funding shortfall. \\\n",
    "            After desperately putting feelers out among their contacts for financing, Longoria was the candidate to put her hand up, investing $US6 million of her own money \\\n",
    "            to save the project.\"\n",
    "\n",
    "context3 = \"As I was going to St. Ives,\\\n",
    "            I met a man with seven wives.\\\n",
    "            Every wife had seven sacks,\\\n",
    "            Every sack had seven cats,\\\n",
    "            Every cat had seven kits:\\\n",
    "            Kits, cats, sacks, and wives.\\\n",
    "            How many were going to St. Ives?\"\n",
    "\n",
    "context4 = \"\"\"Federal government agencies are set to bring $49 million worth of technology services in-house as part of a push to cut contractor numbers in the public service.\n",
    "              Figures released today by Finance Minister Katy Gallagher reveal that $527 million worth of “core work” is to be brought back in-house in 2024-25 across 104 government agencies.\n",
    "              The announcement comes a year after Gallagher first revealed the government’s push to phase out contractors and consultants in the Australian Public Service via the Strategic Commissioning Framework.\n",
    "              According to an update to the framework, which includes an overview of each agency’s 2024-25 target, ICT and digital will account for 22 percent of this core work, with the exception of Defence.\n",
    "              Defence itself bore the biggest brunt of the outsourcing cuts with a total reduction of $308 million but did not break down specifics of which work would move in-house.\n",
    "              However, in the recent iTnews Podcast, Defence CIO Chris Crozier revealed that the department’s tech delivery was now at a 60:40 ratio of staff to contractors, down from 20:80.\n",
    "              The Australian Taxation Office earlier revealed it would be looking to reduce $31.9 million in 2024–25 in outsourcing expenditure for IT, service delivery and data analytics work.\n",
    "              In the framework report, 67 departments and agencies identified the grouping of 'ICT and digital' as core systems, with 55 of these outsourcing at least some part of it.\n",
    "              “Agencies report widespread outsourcing of core work in this job family and note it is difficult to bring in-house,” the report stated.\n",
    "              Since taking office in 2022, Gallagher has made it her mission to reduce dependency on consultants and contractors in the APS, especially following an earlier senate finding that the APS had an \"unhealthy reliance on IT contractors\".\n",
    "              “When coming to government we set out with an ambitious agenda to reform the APS, and to strengthen capability, to ensure the APS can deliver the services Australians expect,” Gallagher said in a statement.\"\"\"\n",
    "\n",
    "#prompt = \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\"\n",
    "#prompt = \"Who is the current America president?\"\n",
    "#prompt = \"Who will win the up coming presidential election in America?\"\n",
    "#prompt = \"Is it safe to go swimming straight after eating?\"\n",
    "#prompt = \"In a very concise approach, can you tell me the difference between traditional Machine learning and Generative AI?\"\n",
    "#prompt = \"Is it ever justified to break the law if it saves a life?\"\n",
    "#prompt = \"Who is Elara and is she an soccer player or archaeologist?\"\n",
    "#prompt = \"Can you verify if Elara made any purchases? If so, how much money did she start with and how much did she have in the end?\"\n",
    "#prompt = \"Can you summarise the this information and provide the summary as 3 key bullet points?\"\n",
    "#prompt = \"Is it fair to say that Leo is much smarter than Elara and that is probably becuase he is at least 5 years older than her?\"\n",
    "#prompt = \"How many people are mentioned in this passage?\"\n",
    "#prompt = \"How many were going to St. Ives?\"\n",
    "prompt = \"What is the key themse of this text??\"\n",
    "prompt = prompt + context4\n",
    "\n",
    "# This is a deliberate attempt to get the answer to be of poor quality and see if that is reflected in the score\n",
    "misdirection = \"\"\"you are to deliberately say the opposite of what you find as the answer, i.e. if you can see the correct answer is up, then you must say the answer is down.\n",
    "                  Additionally, you are to include information that has nothing to do with this topic at hand, i.e. if the question is about soccer, you are to talk about fishing\"\"\"\n",
    "prompt = misdirection + prompt\n",
    "# Comment out the above section if trying to get proper scores.\n",
    "\n",
    "# -- Part 1a: this is vanilla call to llm\n",
    "print('-' * 15, 'part 1', '-' * 15)\n",
    "# WIP: recreate part 1\n",
    "# Replaces previous version. This one adds temperature\n",
    "# --- Ground truth calls from llm ---\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key (replace with your actual key)\n",
    "#genai.configure(api_key= )\n",
    "\n",
    "# Define your question as a variable\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Start chat session\n",
    "chat_session = model.start_chat(history=[])\n",
    "response = chat_session.send_message(prompt)\n",
    "answer = response.text\n",
    "#print(response.text)\n",
    "question = prompt\n",
    "print('-' * 10, 'Q&A','-' * 10 )\n",
    "print('Question:', question)\n",
    "print('Answer:', answer)\n",
    "print('-' * 10, 'Q&A','-' * 10 )\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -- Part 1b: (need to find a way to do without this), but at this time this seems to be required for the 'customer_llm' bit\n",
    "# WIP: recreate part 1\n",
    "# Replaces previous version. This one adds temperature\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# Define your schema class here\n",
    "class YourSchemaClass(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=self.api_key)  # Configure the API key\n",
    "\n",
    "        # Define generation configuration with temperature\n",
    "        generation_config = {\n",
    "            \"temperature\": 1,  # Set your desired temperature here\n",
    "            \"max_output_tokens\": 8192,\n",
    "            \"top_p\": 0.95,\n",
    "             \"top_k\": 64,\n",
    "            \"response_mime_type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"models/gemini-1.5-flash\",\n",
    "            generation_config=generation_config  # Pass the config here\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\"\n",
    "\n",
    "# Now you can use your API key to create an instance\n",
    "#api_key =  # Replace with your actual API key\n",
    "custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -- Part 2:\n",
    "print('-' * 15, 'part 2', '-' * 15)\n",
    "# As per above, but reconfiguring so question is asked once only\n",
    "# Eval Two (from DeepEval*)\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "question = question\n",
    "actual_output = answer\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.75,\n",
    "    model=custom_llm,\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    #input=\"Who is the name of the English Premier League team, based out of London whose logo is a cannon?\",\n",
    "    #input=\"Has Chelase won the FA Cup the most any team has won the FA Cup?\",\n",
    "    input=question,\n",
    "    actual_output=actual_output\n",
    ")\n",
    "\n",
    "# Measure and print the score and reason\n",
    "metric.measure(test_case)\n",
    "print('1.0 Metric score:', metric.score)\n",
    "print('1.1 Metric reason:',metric.reason)\n",
    "print('\\n')\n",
    "\n",
    "# Optionally evaluate test cases in bulk\n",
    "#evaluate([test_case], [metric]) # prints out much of the same and not required atm as single response testing in set up\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -- Part 2a:\n",
    "# Source: https://docs.confident-ai.com/docs/guides-rag-evaluation\n",
    "print('-' * 10, 'part 2a', '-' * 10)\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "contextual_recall = ContextualRecallMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "contextual_relevancy = ContextualRelevancyMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "test_case = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=actual_output,\n",
    "    #expected_output=\"the answer is one\", # this goes with context3\n",
    "    expected_output=\"\",\n",
    "    retrieval_context = [context4] # have to convert format\n",
    "    # retrieval_context=[\"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
    "    #                       your degree, unless you have applied for and been approved to participate in OPT.\"\"\"]\n",
    ")\n",
    "\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"2.0 Precision score: \", contextual_precision.score)\n",
    "print(\"2.1 Precision reason: \", contextual_precision.reason)\n",
    "print('\\n')\n",
    "\n",
    "# contextual_recall.measure(test_case)\n",
    "# print(\"3.0 Recall score: \", contextual_recall.score)\n",
    "# print(\"3.1 Recall reason: \", contextual_recall.reason)\n",
    "# print('\\n')\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"4.0 Relevancy score: \", contextual_relevancy.score)\n",
    "print(\"4.1 Relevancy reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbEcT6KG9ut8"
   },
   "outputs": [],
   "source": [
    "#evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhl8gwKkXVG0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ak4cxEX-XVAZ"
   },
   "outputs": [],
   "source": [
    "# Want to experiment with red teaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "CIVgC8T9XU50",
    "outputId": "b584ab55-935c-4298-a7fb-547125c1464a"
   },
   "outputs": [],
   "source": [
    "# Source: https://docs.confident-ai.com/docs/red-teaming-introduction\n",
    "\n",
    "#api_key =   # Replace with your actual API key\n",
    "#custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "\n",
    "from deepeval.red_teaming import RedTeamer\n",
    "\n",
    "target_purpose = \"Provide financial advice, investment suggestions, and answer user queries related to personal finance and market trends.\"\n",
    "target_system_prompt = \"You are a financial assistant designed to help users with financial planning, investment advice, and market analysis. Ensure accuracy, professionalism, and clarity in all responses.\"\n",
    "\n",
    "red_teamer = RedTeamer(\n",
    "    target_purpose=target_purpose,\n",
    "    target_system_prompt=target_system_prompt,\n",
    "    synthesizer_model=CustomGeminiFlash(api_key=api_key)\n",
    ")\n",
    "\n",
    "#target_model=TargetLLM(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pES-CehNXUze",
    "outputId": "4befdc42-ce5f-4755-cbf2-6b95a30cc671"
   },
   "outputs": [],
   "source": [
    "help(RedTeamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD42nop-XUkn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YITCWfmh9tf-"
   },
   "outputs": [],
   "source": [
    "# Want to experiment with new metrics below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JiWlRMdgv82U"
   },
   "outputs": [],
   "source": [
    "# Can delete below. Was trialing, but alredy moved above\n",
    "# Source: https://docs.confident-ai.com/docs/guides-rag-evaluation\n",
    "\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric\n",
    ")\n",
    "\n",
    "contextual_precision = ContextualPrecisionMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "contextual_recall = ContextualRecallMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "contextual_relevancy = ContextualRelevancyMetric(threshold=0.75, model=custom_llm, include_reason=True)\n",
    "\n",
    "# (using this from the earlier code)\n",
    "test_case = LLMTestCase(\n",
    "    input=\"I'm on an F-1 visa, gow long can I stay in the US after graduation?\",\n",
    "    actual_output=\"You can stay up to 30 days after completing your degree.\",\n",
    "    expected_output=\"You can stay up to 60 days after completing your degree.\",\n",
    "    retrieval_context=[\n",
    "        \"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
    "        your degree, unless you have applied for and been approved to participate in OPT.\"\"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# # from deepeval.test_case import LLMTestCase\n",
    "# # test_case = LLMTestCase(\n",
    "# #     input=\"I'm on an F-1 visa, gow long can I stay in the US after graduation?\",\n",
    "# #     actual_output=\"You can stay up to 30 days after completing your degree.\",\n",
    "# #     expected_output=\"You can stay up to 60 days after completing your degree.\",\n",
    "# #     retrieval_context=[\n",
    "# #         \"\"\"If you are in the U.S. on an F-1 visa, you are allowed to stay for 60 days after completing\n",
    "# #         your degree, unless you have applied for and been approved to participate in OPT.\"\"\"\n",
    "# #     ]\n",
    "# # )\n",
    "\n",
    "contextual_precision.measure(test_case)\n",
    "print(\"Score: \", contextual_precision.score)\n",
    "print(\"Reason: \", contextual_precision.reason)\n",
    "\n",
    "contextual_recall.measure(test_case)\n",
    "print(\"Score: \", contextual_recall.score)\n",
    "print(\"Reason: \", contextual_recall.reason)\n",
    "\n",
    "contextual_relevancy.measure(test_case)\n",
    "print(\"Score: \", contextual_relevancy.score)\n",
    "print(\"Reason: \", contextual_relevancy.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dFckVXEzE6x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i49qARP6v8w3"
   },
   "outputs": [],
   "source": [
    "prompt = \"who is the current American president\"\n",
    "response = chat_session.send_message(prompt)\n",
    "print(response. text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-utfXvhv8sQ"
   },
   "outputs": [],
   "source": [
    "# Testing if i can clean up the Deepeval output to not include all the text at the and that is adding no value.\n",
    "\n",
    "def remove_substring_and_after(text, substring):\n",
    "    # Find the index of the substring\n",
    "    index = text.find(substring)\n",
    "\n",
    "    # If the substring is found, slice the text up to that index\n",
    "    if index != -1:\n",
    "        return text[:index]\n",
    "\n",
    "    # If the substring is not found, return the original text\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "original_text = \"This is a sample text. Remove everything after this.\"\n",
    "substring_to_remove = \"Remove\"\n",
    "result = remove_substring_and_after(original_text, substring_to_remove)\n",
    "print(result)  # Output: \"This is a sample text. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RqBrVtPJv8mk"
   },
   "outputs": [],
   "source": [
    "print('Metric score:', metric.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7KGaDPGFVGC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvQjO5YN2Pq6"
   },
   "outputs": [],
   "source": [
    "# -- trying to validate above. It seems when i ask the model a question the answer is ok, but when i repeat that in deepeval, answer is wrong?\n",
    "question = \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\"\n",
    "\n",
    "# Generate output from the LLM using the class as schema\n",
    "actual_output_instance = custom_llm.generate(\n",
    "    question,\n",
    "    schema=YourSchemaClass  # Pass the class, not an instance\n",
    ")\n",
    "\n",
    "# Extract the response from the actual_output_instance\n",
    "actual_output = actual_output_instance.response  # Get the response string\n",
    "print(actual_output_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7DSxwwr2Pny"
   },
   "outputs": [],
   "source": [
    "# WIP: recreate part 1\n",
    "# Replaces previous version. This one adds temperature\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# Define your schema class here\n",
    "class YourSchemaClass(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=self.api_key)  # Configure the API key\n",
    "        # Define generation configuration with temperature\n",
    "        generation_config = {\n",
    "            \"temperature\": 0,  # Set your desired temperature here\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 8,\n",
    "            \"max_output_tokens\": 8192\n",
    "            #\"response_mime_type\": \"application/json\",\n",
    "        }\n",
    "        self.model = genai.GenerativeModel(\n",
    "            #model_name=\"models/gemini-1.5-flash\",\n",
    "            model_name=\"gemini-1.5-flash\",\n",
    "            generation_config=generation_config  # Pass the config here\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        #return \"Gemini 1.5 Flash\"\n",
    "        return \"gemini-1.5-flash\"\n",
    "\n",
    "# Now you can use your API key to create an instance\n",
    "#api_key =   # Replace with your actual API key\n",
    "custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "response = custom_llm.generate(\"Has Chelsea won the FA Cup the most any team has won the FA Cup?\", schema=YourSchemaClass)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Ep7ToWzUJ0x"
   },
   "outputs": [],
   "source": [
    "# Still trying to ge above code (code 1) to output correct answer.\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "# Define your schema class here\n",
    "class YourSchemaClass(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        genai.configure(api_key=self.api_key)  # Configure the API key\n",
    "        # Define generation configuration with temperature\n",
    "        generation_config = {\n",
    "            \"temperature\": 0.2,  # Adjusted temperature\n",
    "            \"top_p\": 0.95,\n",
    "            \"top_k\": 8,\n",
    "            \"max_output_tokens\": 8192\n",
    "        }\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=\"gemini-1.5-flash\"\n",
    "            generation_config=generation_config  # Pass the config here\n",
    "        )\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_model=schema,\n",
    "        )\n",
    "\n",
    "        # Debug: Print raw response\n",
    "        print(\"Raw response:\", resp)\n",
    "\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"gemini-1.5-flash\"\n",
    "\n",
    "# Now you can use your API key to create an instance\n",
    "#api_key =   # Replace with your actual API key\n",
    "custom_llm = CustomGeminiFlash(api_key=api_key)\n",
    "\n",
    "# Example usage\n",
    "response = custom_llm.generate(\"Has Chelsea won the FA Cup the most any team has won the FA Cup?\", schema=YourSchemaClass)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtSr6CQVVRHJ"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define a simple schema for the response\n",
    "class SimpleResponseModel(BaseModel):\n",
    "    content: str\n",
    "\n",
    "# Function to call the Gemini API\n",
    "def call_gemini_api(api_key: str, prompt: str):\n",
    "    # Configure the API key\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # Load the model\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash\"\n",
    "    )\n",
    "\n",
    "    # Create an instructor client\n",
    "    instructor_client = instructor.from_gemini(\n",
    "        client=model,\n",
    "        mode=instructor.Mode.GEMINI_JSON,\n",
    "    )\n",
    "\n",
    "    # Generate response\n",
    "    response = instructor_client.messages.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=SimpleResponseModel  # Specify the response model\n",
    "    )\n",
    "\n",
    "    # Print raw response\n",
    "    print(\"Raw response:\", response)\n",
    "\n",
    "# Example usage\n",
    "#api_key =  # Replace with your actual API key\n",
    "call_gemini_api(api_key, \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6SM8nOlWuLD"
   },
   "outputs": [],
   "source": [
    "!pip install jsonref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ulq9yTMaWKr-"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import instructor\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define a simple schema for the response\n",
    "class SimpleResponseModel(BaseModel):\n",
    "    content: str\n",
    "\n",
    "# Function to call the Gemini API\n",
    "def call_gemini_api(api_key: str, prompt: str):\n",
    "    # Configure the API key\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "    # Load the model\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "    # Create an instructor client\n",
    "    instructor_client = instructor.from_gemini(client=model)\n",
    "\n",
    "    # Generate response\n",
    "    response = instructor_client.messages.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_model=SimpleResponseModel\n",
    "    )\n",
    "\n",
    "    # Print raw response\n",
    "    print(\"Raw response:\", response)\n",
    "\n",
    "# Example usage\n",
    "#api_key =  # Replace with your actual API key\n",
    "call_gemini_api(api_key, \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON0WYv7GWKhd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gr00fRM2PkW"
   },
   "outputs": [],
   "source": [
    "# -- this is a recreation of code 1, but trying to get it to output the result in the format of actual code 1:\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key (replace with your actual key)\n",
    "#api_key = \n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Define your schema class here\n",
    "class ResponseSchema(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "# Define your generation configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Start chat session\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "# Define your prompt\n",
    "prompt = \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\"\n",
    "response = chat_session.send_message(prompt)\n",
    "\n",
    "# Wrap the response in the schema and print it\n",
    "formatted_response = ResponseSchema(response=response.text)\n",
    "print(formatted_response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8AMjxDM92PhI"
   },
   "outputs": [],
   "source": [
    "# Turning above into a function:import os\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key (replace with your actual key)\n",
    "#api_key = \n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# Define your schema class here\n",
    "class ResponseSchema(BaseModel):\n",
    "    response: str  # Define the fields you expect in the response\n",
    "\n",
    "# Define your generation configuration\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Start chat session\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "def get_response(question: str) -> str:\n",
    "    \"\"\"Send a question to the chat session and return the formatted response.\"\"\"\n",
    "    response = chat_session.send_message(question)\n",
    "\n",
    "    # Wrap the response in the schema and return it as JSON\n",
    "    formatted_response = ResponseSchema(response=response.text)\n",
    "    return formatted_response.json()\n",
    "\n",
    "# Example usage\n",
    "question = \"Has Chelsea won the FA Cup the most any team has won the FA Cup?\"\n",
    "print(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIqy8Z-I2PeB"
   },
   "outputs": [],
   "source": [
    "custom_llm = get_response(question)\n",
    "#response = custom_llm.generate(\"Has Chelsea won the FA Cup the most any team has won the FA Cup?\", schema=YourSchemaClass)\n",
    "print(custom_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuCcbD742Pbf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRh9l9Gp2PYT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7vCIMzeakv8"
   },
   "outputs": [],
   "source": [
    "# Below is test (from earlier than above) and is archived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi2_XTaTakgu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MG-4RLfbL53I"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set your API key here\n",
    "#genai.configure(api_key= )\n",
    "\n",
    "from pydantic import BaseModel\n",
    "import google.generativeai as genai\n",
    "import instructor\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.models import DeepEvalBaseLLM, LLMTestCase\n",
    "\n",
    "\n",
    "# Define a schema for the response\n",
    "class JokeResponseSchema(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "class CustomGeminiFlash(DeepEvalBaseLLM):\n",
    "    def __init__(self):\n",
    "        self.model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        client = self.load_model()\n",
    "        instructor_client = instructor.from_gemini(\n",
    "            client=client,\n",
    "            mode=instructor.Mode.GEMINI_JSON,\n",
    "        )\n",
    "        resp = instructor_client.messages.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            response_model=schema,\n",
    "        )\n",
    "        return resp\n",
    "\n",
    "    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:\n",
    "        return self.generate(prompt, schema)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Gemini 1.5 Flash\"\n",
    "\n",
    "custom_llm = CustomGeminiFlash()\n",
    "\n",
    "# Generate a joke response\n",
    "generated_response = custom_llm.generate(\"Write me a joke\", schema=JokeResponseSchema)\n",
    "\n",
    "# Extract the joke text from the response\n",
    "joke_text = generated_response.joke if isinstance(generated_response, JokeResponseSchema) else \"\"\n",
    "\n",
    "# Define the ground truth for evaluation\n",
    "ground_truth = \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    "\n",
    "# Create an LLMTestCase instance\n",
    "test_case = LLMTestCase(\n",
    "    input=joke_text,\n",
    "    output=ground_truth,\n",
    "    model=custom_llm\n",
    ")\n",
    "\n",
    "# Initialize the metric\n",
    "metric = AnswerRelevancyMetric(model=custom_llm)\n",
    "\n",
    "# Measure the relevancy\n",
    "score = metric.measure(test_case)  # Pass the LLMTestCase instance\n",
    "\n",
    "print(f\"Relevancy Score: {score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Iom2PpEIG_Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJ_wWI11w1YR"
   },
   "outputs": [],
   "source": [
    "# Install the Google AI Python SDK (if not already installed)\n",
    "# !pip install google-generativeai\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "#genai.configure(api_key= )\n",
    "\n",
    "question = \"Can provide 3 bullet points on AI in the workplace?\"  # You can change this to any question\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Start chat session\n",
    "chat_session = model.start_chat(history=[])\n",
    "# -- Adding prompt: 02.06 10.56 -------\n",
    "# Define your instructional prompt here\n",
    "instructional_prompt = \"**You are a helpful agent, who answers questions and formats the reponse in a professional way.** \"\n",
    "\n",
    "# Combine prompt and text_to_spell\n",
    "prompt = instructional_prompt + question\n",
    "# -- Adding prompt: 02.06 10.56 -------\n",
    "# Send the question variable to the LLM\n",
    "response = chat_session.send_message(prompt)\n",
    "# Print the generated response\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJzHoI3wwNuy"
   },
   "outputs": [],
   "source": [
    "pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1ZtTU-VwNrw"
   },
   "outputs": [],
   "source": [
    "#genai.configure(api_key=)\n",
    "\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "from deepeval import evaluate  # Ensure you have the correct import\n",
    "\n",
    "# Configure the Google Generative AI\n",
    "# genai.configure(api_key='')  # Uncomment and set your API key\n",
    "\n",
    "# Define the question\n",
    "question = \"Can you provide 3 bullet points on AI in the workplace?\"\n",
    "\n",
    "# Configuration for response generation\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Safety settings for the model\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create the model instance\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Start chat session\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "# Define the instructional prompt\n",
    "instructional_prompt = \"**You are a helpful agent, who answers questions and formats the response in a professional way.** \"\n",
    "\n",
    "# Combine prompt and question\n",
    "prompt = instructional_prompt + question\n",
    "\n",
    "# Send the question to the LLM and get the response\n",
    "response = chat_session.send_message(prompt)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"Generated Response:\")\n",
    "print(response.text)\n",
    "\n",
    "# Prepare evaluation criteria\n",
    "reference_response = (\n",
    "    \"1. AI can automate repetitive tasks.\\n\"\n",
    "    \"2. AI can enhance decision-making through data analysis.\\n\"\n",
    "    \"3. AI can facilitate remote work and collaboration.\"\n",
    ")\n",
    "\n",
    "# Define metrics functions\n",
    "def completeness(prediction, reference):\n",
    "    return 0.9  # Dummy return value\n",
    "\n",
    "def clarity(prediction, reference):\n",
    "    return 0.8  # Dummy return value\n",
    "\n",
    "# Define metrics as a list of functions\n",
    "metrics = [completeness, clarity]\n",
    "\n",
    "# Perform the evaluation\n",
    "try:\n",
    "    # Inspect input types\n",
    "    print(\"Response type:\", type(response.text))\n",
    "    print(\"Reference type:\", type(reference_response))\n",
    "\n",
    "    # Perform evaluation with functions\n",
    "    evaluation_results = evaluate(\n",
    "        predictions=[response.text],  # Predictions as a list\n",
    "        references=[reference_response],  # References as a list\n",
    "        metrics=metrics  # Metrics as a list of functions\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# Check if evaluation_results was defined and valid\n",
    "if 'evaluation_results' in locals() and evaluation_results is not None:\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(evaluation_results)\n",
    "else:\n",
    "    print(\"Evaluation was not successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdowGJxL9D3D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoaWXcz69DLV"
   },
   "outputs": [],
   "source": [
    "pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLRXga-DwNoT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from deepeval import evaluate  # Ensure you have the correct import\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "\n",
    "# Configure the Google Generative AI\n",
    "# genai.configure(api_key='YOUR_API_KEY_HERE')  # Uncomment and set your API key\n",
    "\n",
    "# Define the question\n",
    "question = \"Can you provide 3 bullet points on AI in the workplace?\"\n",
    "\n",
    "# Configuration for response generation\n",
    "generation_config = {\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "# Safety settings for the model\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create the model instance\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    safety_settings=safety_settings,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# Start chat session\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "# Define the instructional prompt\n",
    "instructional_prompt = \"**You are a helpful agent, who answers questions and formats the response in a professional way.** \"\n",
    "\n",
    "# Combine prompt and question\n",
    "prompt = instructional_prompt + question\n",
    "\n",
    "# Send the question to the LLM and get the response\n",
    "response = chat_session.send_message(prompt)\n",
    "\n",
    "# Print the generated response\n",
    "print(\"Generated Response:\")\n",
    "print(response.text)\n",
    "\n",
    "# Prepare evaluation criteria\n",
    "reference_response = (\n",
    "    \"1. AI can automate repetitive tasks.\\n\"\n",
    "    \"2. AI can enhance decision-making through data analysis.\\n\"\n",
    "    \"3. AI can facilitate remote work and collaboration.\"\n",
    ")\n",
    "\n",
    "# Define metrics functions\n",
    "def completeness(prediction, reference):\n",
    "    return 0.9  # Dummy return value\n",
    "\n",
    "def clarity(prediction, reference):\n",
    "    return 0.8  # Dummy return value\n",
    "\n",
    "def f1_metric(prediction, reference):\n",
    "    prediction_tokens = set(prediction.split())\n",
    "    reference_tokens = set(reference.split())\n",
    "    true_positive = len(prediction_tokens.intersection(reference_tokens))\n",
    "    precision = true_positive / len(prediction_tokens) if prediction_tokens else 0\n",
    "    recall = true_positive / len(reference_tokens) if reference_tokens else 0\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "def bleu_metric(prediction, reference):\n",
    "    reference_tokens = reference.split()\n",
    "    prediction_tokens = prediction.split()\n",
    "    return sentence_bleu([reference_tokens], prediction_tokens)\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "def rouge_metric(prediction, reference):\n",
    "    scores = rouge.get_scores(prediction, reference)\n",
    "    return scores[0]['rouge-1']['f']  # Return the F1 score for ROUGE-1\n",
    "\n",
    "def distinct_n_grams(prediction, n=2):\n",
    "    prediction_tokens = prediction.split()\n",
    "    n_grams = set(tuple(prediction_tokens[i:i+n]) for i in range(len(prediction_tokens)-n+1))\n",
    "    return len(n_grams) / len(prediction_tokens) if prediction_tokens else 0\n",
    "\n",
    "# Define metrics as a list of functions\n",
    "metrics = [completeness, clarity, f1_metric, bleu_metric, rouge_metric, distinct_n_grams]\n",
    "\n",
    "# Perform the evaluation\n",
    "try:\n",
    "    print(\"Response type:\", type(response.text))\n",
    "    print(\"Reference type:\", type(reference_response))\n",
    "\n",
    "    # Perform evaluation with the new metrics\n",
    "    evaluation_results = evaluate(\n",
    "        predictions=[response.text],\n",
    "        references=[reference_response],\n",
    "        metrics=metrics\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "\n",
    "# Check if evaluation_results was defined and valid\n",
    "if 'evaluation_results' in locals() and evaluation_results is not None:\n",
    "    print(\"Evaluation Results:\")\n",
    "    print(evaluation_results)\n",
    "else:\n",
    "    print(\"Evaluation was not successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3czYko4wNCP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gx-BJszyPYPa"
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Zej8qd_PKyJ"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "\n",
    "import requests\n",
    "\n",
    "#api_key = \n",
    "headers = {\n",
    "    'Authorization': f'Bearer {api_key}',\n",
    "}\n",
    "\n",
    "response = requests.get('https://api.openai.com/v1/models', headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGo5MYyySiaJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ...',\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'model': 'gpt-3.5-turbo-instruct',\n",
    "    'messages': [{'role': 'user', 'content': 'Hello, how can I test you?'}],\n",
    "    'max_tokens': 100,\n",
    "}\n",
    "\n",
    "response = requests.post('https://api.openai.com/v1/chat/completions', headers=headers, json=data)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJOs5biFQq-M"
   },
   "outputs": [],
   "source": [
    "#!pip install langchain-openai\n",
    "#!pip install datasets\n",
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HoQY0KgPriV"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"explodinggradients/amnesty_qa\",\"english_v3\")\n",
    "\n",
    "from ragas import EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"eval\"])\n",
    "\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "\n",
    "import os\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \n",
    "#os.environ[\"\"] = \n",
    "\n",
    "\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "#evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)\n",
    "\n",
    "df = results.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yux_pX15Sw2s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKeQ3uU0S7_D"
   },
   "outputs": [],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4NmPiA2Swre"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ragas import EvaluationDataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "\n",
    "# Create a dummy dataset\n",
    "dummy_data = {\n",
    "    \"question\": [\"What is AI?\", \"What is machine learning?\", \"What is deep learning?\"],\n",
    "    \"answer\": [\"AI stands for Artificial Intelligence.\",\n",
    "               \"Machine learning is a subset of AI that focuses on data.\",\n",
    "               \"Deep learning is a type of machine learning using neural networks.\"],\n",
    "    \"id\": [1, 2, 3]\n",
    "}\n",
    "\n",
    "# Convert the dummy data into a DataFrame\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Create an evaluation dataset from the dummy DataFrame\n",
    "eval_dataset = EvaluationDataset.from_pandas(dummy_df)\n",
    "\n",
    "# Initialize the evaluator LLM and embeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# Define metrics for evaluation\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "\n",
    "# Evaluate using the dummy dataset and defined metrics\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)\n",
    "\n",
    "# Display the results as a DataFrame\n",
    "df = results.to_pandas()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKkcqGlRTSfE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDWTGl8UTSMW"
   },
   "outputs": [],
   "source": [
    "os.environ[] \n",
    "\n",
    "# ------------------\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate\n",
    "\n",
    "# Create a dummy dataset with additional required columns\n",
    "dummy_data = {\n",
    "    \"question\": [\"What is AI?\", \"What is machine learning?\", \"What is deep learning?\"],\n",
    "    \"answer\": [\"AI stands for Artificial Intelligence.\",\n",
    "               \"Machine learning is a subset of AI that focuses on data.\",\n",
    "               \"Deep learning is a type of machine learning using neural networks.\"],\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"retrieved_contexts\": [\n",
    "        [\"AI is a field of study.\"],\n",
    "        [\"Machine learning involves algorithms.\"],\n",
    "        [\"Deep learning uses neural networks.\"],\n",
    "    ],\n",
    "    \"reference\": [\n",
    "        \"AI is a field of study that simulates human intelligence.\",\n",
    "        \"Machine learning is a branch of AI focused on data-driven predictions.\",\n",
    "        \"Deep learning is a subset of machine learning using layered neural networks.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dummy data into a DataFrame\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "\n",
    "# Create an evaluation dataset from the dummy DataFrame using Hugging Face's Dataset\n",
    "eval_dataset = Dataset.from_pandas(dummy_df)\n",
    "\n",
    "# Initialize the evaluator LLM and embeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
    "#evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo-0301\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n",
    "\n",
    "# Define metrics for evaluation\n",
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm),\n",
    "    FactualCorrectness(llm=evaluator_llm),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "\n",
    "# Evaluate using the dummy dataset and defined metrics\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)\n",
    "\n",
    "# Display the results as a DataFrame\n",
    "df = results.to_pandas()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BMGLhSW9WQL9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3NujzfJ6W9VR",
    "u9fhnk98XGZP"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ce3fbe84f7843c0986334449e09026b": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_a596db509621467db7b2ec981aff3810",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "4bc612e810b14f5aa121d3b4bcee37a6": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_b76a87649aa2448b8d96e7872d5b9f47",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode=Tru…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode=Tru…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "73fad46582594510ae8a92ec6a35d409": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_bde9cd1bd86f4d7f947b090c1a94cf7b",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "a596db509621467db7b2ec981aff3810": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b76a87649aa2448b8d96e7872d5b9f47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bde9cd1bd86f4d7f947b090c1a94cf7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
