{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 18.02.25\n",
    "# Purpose: Testing manual set up of LLM as a judge v2\n",
    "# Theme: Eval\n",
    "# Status: WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: incudes experiments with retrival (hybrid search) before LLM as judge (see below for reasons)\n",
    "\n",
    "#Terms of reference: \n",
    "#1. **Relevance:** Does the response directly answer the query?\n",
    "#2. **Faithfulness:** Is the response factually grounded in the provided documents?\n",
    "#3. **Completeness:** Does the response cover all necessary details?\n",
    "#4. **Conciseness:** Is the response free from unnecessary information?\n",
    "#5. **Coherence:** Is the response well-structured and readable?\n",
    "\n",
    "#Status checking: A logical (and common) approach is to compare the answer to the retrieved chunks to check for **Faithfulness**.\n",
    "#That obviously pre-supposes the retrieved chunks hold the information to answer the question (otherwise comparison of retrieved to \n",
    "#answer means nothing). \n",
    "\n",
    "#So before starting on assessing **Faithfulness**, steps need to be taken to validate the retrieved chunks.\n",
    "\n",
    "#Here are some steps to try to improve Chunk Recall\n",
    "#- 1.0 Chunk Overlap & Adaptive Sizing\n",
    "#Use overlapping chunks to reduce cases where key information is split across boundaries.\n",
    "#Dynamically expand chunks if retrieval confidence is low.\n",
    "\n",
    "#- 2.0 Hybrid Retrieval (BM25 + Embeddings)\n",
    "#Combine dense vector search (embeddings) with BM25 (keyword-based) to improve recall.\n",
    "#This balances semantic and exact-match retrieval.\n",
    "\n",
    "#- 3.0 Query Expansion / Rewriting\n",
    "#Automatically rewrite the query to include synonyms or more context.\n",
    "#Use an LLM to generate multiple variations of the query.\n",
    "\n",
    "#- 4.0 Multi-Hop Retrieval\n",
    "#If a query requires multiple steps (e.g., cross-referencing different sections), use iterative retrieval instead of single-pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4kQkGunYC_wi",
    "outputId": "099236e2-701a-4a88-cc24-20cba3f076ea"
   },
   "outputs": [],
   "source": [
    "# Lib:\n",
    "# --- Install lib (1st set is for rag)\n",
    "##!pip3 install --upgrade google-cloud-aiplatform\n",
    "##!pip3 install ipython pandas[output_formatting] google-cloud-language==2.10.0\n",
    "# !pip install langchain # 10.11.24 comment out as not sure using & trying to simplify code.\n",
    "# !pip install langchain-community # 10.11.24 comment out as not sure using & trying to simplify code.\n",
    "\n",
    "!pip install PyPDF2\n",
    "!pip install pypdf\n",
    "!pip install sentence-transformers\n",
    "#!pip install torch\n",
    "#!pip install pandas # 10.11.24 comment out as not sure using & trying to simplify code.\n",
    "!pip install pdfplumber\n",
    "#!pip install llama_index # 10.11.24 comment out as not sure using & trying to simplify code.\n",
    "pip install pandas\n",
    "pip install spacy\n",
    "pip install rank_bm25\n",
    "pip install markdown\n",
    "#pip install google-generativeai langchain-community # wrong\n",
    "pip install langchain-google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "yNlFVcrvDKOU",
    "outputId": "86e75bdb-e2ee-49e4-945a-5e761c87143b"
   },
   "outputs": [],
   "source": [
    "# # RAG:\n",
    "\n",
    "## Includes two parts.\n",
    "# The 2nd part replaced the 1st. Now both commented out and replaced with next cell\n",
    "# Which is the 2nd code here + overlap\n",
    "\n",
    "# Part i\n",
    "# # Spacy1\n",
    "# # -- Do NOT Delete. Splitting by sentence and maintaining chunk size. --\n",
    "\n",
    "# # Trying to replace above - 1st time using spacy.\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import pdfplumber\n",
    "# import spacy\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "# # Load English language model and add sentencizer\n",
    "# nlp = English()\n",
    "# nlp.add_pipe('sentencizer')\n",
    "\n",
    "# def process_pdf(filepath, max_chunk_size=1000):\n",
    "#     \"\"\"Processes a PDF, splits by sentences using spaCy, and returns a DataFrame\"\"\"\n",
    "#     chunks = []\n",
    "#     with pdfplumber.open(filepath) as pdf:\n",
    "#         for page_number, page in enumerate(pdf.pages):\n",
    "#             page_text = page.extract_text()\n",
    "#             doc = nlp(page_text)  # Process text with spaCy\n",
    "#             sentences = [str(sent).strip() for sent in doc.sents]  # Extract sentences\n",
    "\n",
    "#             chunk_text = \"\"\n",
    "#             for sentence in sentences:\n",
    "#                 # Combine sentences within chunk limit\n",
    "#                 if len(chunk_text + sentence) <= max_chunk_size:\n",
    "#                     chunk_text += \" \" + sentence\n",
    "#                 else:\n",
    "#                     # Add current chunk and reset\n",
    "#                     chunks.append({\n",
    "#                         \"page_number\": page_number+1,  # Page number is 1-indexed\n",
    "#                         \"chunk_text\": chunk_text.strip(),\n",
    "#                         \"chunk_length\": len(chunk_text.strip())\n",
    "#                     })\n",
    "#                     chunk_text = sentence\n",
    "#             # Add remaining text in the last chunk\n",
    "#             if chunk_text:\n",
    "#                 chunks.append({\n",
    "#                     \"page_number\": page_number+1,\n",
    "#                     \"chunk_text\": chunk_text.strip(),\n",
    "#                     \"chunk_length\": len(chunk_text.strip())\n",
    "#                 })\n",
    "#     return pd.DataFrame(chunks)\n",
    "\n",
    "# def process_pdf_directory(directory_path, max_chunk_size=1000):\n",
    "#     \"\"\"Processes all PDFs in a directory and combines results into a single DataFrame\"\"\"\n",
    "#     all_chunks = []\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         if filename.endswith(\".pdf\"):\n",
    "#             file_path = os.path.join(directory_path, filename)\n",
    "#             pdf_df = process_pdf(file_path, max_chunk_size)\n",
    "#             all_chunks.append(pdf_df)\n",
    "#     return pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# # Example usage\n",
    "# #directory_path = \"/kaggle/input/payment-hltc\"\n",
    "# directory_path = \"/content/Untitled Folder\"\n",
    "# max_chunk_size = 1000\n",
    "# all_chunks_df = process_pdf_directory(directory_path, max_chunk_size)\n",
    "\n",
    "# display(all_chunks_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Part ii\n",
    "# # -- Replaces above: Key difference being this includes the doc name:\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import pdfplumber\n",
    "# import spacy\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "# # Load English language model and add sentencizer\n",
    "# nlp = English()\n",
    "# nlp.add_pipe('sentencizer')\n",
    "\n",
    "# def process_pdf(filepath, doc_name, max_chunk_size=1000):\n",
    "#     \"\"\"Processes a PDF, splits by sentences using spaCy, and returns a DataFrame\"\"\"\n",
    "#     chunks = []\n",
    "#     with pdfplumber.open(filepath) as pdf:\n",
    "#         for page_number, page in enumerate(pdf.pages):\n",
    "#             page_text = page.extract_text()\n",
    "#             doc = nlp(page_text)  # Process text with spaCy\n",
    "#             sentences = [str(sent).strip() for sent in doc.sents]  # Extract sentences\n",
    "\n",
    "#             chunk_text = \"\"\n",
    "#             for sentence in sentences:\n",
    "#                 # Combine sentences within chunk limit\n",
    "#                 if len(chunk_text + sentence) <= max_chunk_size:\n",
    "#                     chunk_text += \" \" + sentence\n",
    "#                 else:\n",
    "#                     # Add current chunk and reset\n",
    "#                     chunks.append({\n",
    "#                         \"document_name\": doc_name,  # Include document name\n",
    "#                         \"page_number\": page_number + 1,  # Page number is 1-indexed\n",
    "#                         \"chunk_text\": chunk_text.strip(),\n",
    "#                         \"chunk_length\": len(chunk_text.strip())\n",
    "#                     })\n",
    "#                     chunk_text = sentence\n",
    "\n",
    "#             # Add remaining text in the last chunk\n",
    "#             if chunk_text:\n",
    "#                 chunks.append({\n",
    "#                     \"document_name\": doc_name,  # Include document name\n",
    "#                     \"page_number\": page_number + 1,\n",
    "#                     \"chunk_text\": chunk_text.strip(),\n",
    "#                     \"chunk_length\": len(chunk_text.strip())\n",
    "#                 })\n",
    "\n",
    "#     return pd.DataFrame(chunks)\n",
    "\n",
    "# def process_pdf_directory(directory_path, max_chunk_size=1000):\n",
    "#     \"\"\"Processes all PDFs in a directory and combines results into a single DataFrame\"\"\"\n",
    "#     all_chunks = []\n",
    "#     for filename in os.listdir(directory_path):\n",
    "#         if filename.endswith(\".pdf\"):\n",
    "#             file_path = os.path.join(directory_path, filename)\n",
    "#             pdf_df = process_pdf(file_path, filename, max_chunk_size)  # Pass filename as document name\n",
    "#             all_chunks.append(pdf_df)\n",
    "#     return pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# # Example usage\n",
    "# # directory_path = \"/kaggle/input/payment-hltc\"\n",
    "# directory_path = \"data/pdf\"\n",
    "# max_chunk_size = 1000\n",
    "# all_chunks_df = process_pdf_directory(directory_path, max_chunk_size)\n",
    "\n",
    "# display(all_chunks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of the last set of code above, but checking if overlap can be added?\n",
    "import os\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English language model and add sentencizer\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "def process_pdf(filepath, doc_name, max_chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Processes a PDF, splits by sentences using spaCy, and returns a DataFrame with chunk overlap\"\"\"\n",
    "    chunks = []\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page_number, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "            if not page_text:\n",
    "                continue  # Skip empty pages\n",
    "            \n",
    "            doc = nlp(page_text)  # Process text with spaCy\n",
    "            sentences = [str(sent).strip() for sent in doc.sents if sent.text.strip()]  # Extract non-empty sentences\n",
    "\n",
    "            chunk_text = \"\"\n",
    "            overlap_text = \"\"  # Store overlap from previous chunk\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if len(chunk_text + sentence) <= max_chunk_size:\n",
    "                    chunk_text += \" \" + sentence\n",
    "                else:\n",
    "                    # Save chunk and include overlap\n",
    "                    chunks.append({\n",
    "                        \"document_name\": doc_name,\n",
    "                        \"page_number\": page_number + 1,\n",
    "                        \"chunk_text\": chunk_text.strip(),\n",
    "                        \"chunk_length\": len(chunk_text.strip())\n",
    "                    })\n",
    "\n",
    "                    # Set overlap for next chunk (take the last `chunk_overlap` characters)\n",
    "                    overlap_text = chunk_text[-chunk_overlap:] if chunk_overlap > 0 else \"\"\n",
    "                    \n",
    "                    # Start new chunk with overlap\n",
    "                    chunk_text = overlap_text + \" \" + sentence\n",
    "\n",
    "            # Add remaining text in the last chunk\n",
    "            if chunk_text.strip():\n",
    "                chunks.append({\n",
    "                    \"document_name\": doc_name,\n",
    "                    \"page_number\": page_number + 1,\n",
    "                    \"chunk_text\": chunk_text.strip(),\n",
    "                    \"chunk_length\": len(chunk_text.strip())\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(chunks)\n",
    "\n",
    "def process_pdf_directory(directory_path, max_chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Processes all PDFs in a directory and combines results into a single DataFrame\"\"\"\n",
    "    all_chunks = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            pdf_df = process_pdf(file_path, filename, max_chunk_size, chunk_overlap)  # Pass overlap parameter\n",
    "            all_chunks.append(pdf_df)\n",
    "    return pd.concat(all_chunks, ignore_index=True)\n",
    "\n",
    "# Example usage\n",
    "directory_path = \"data/pdf\"\n",
    "max_chunk_size = 1000\n",
    "chunk_overlap = 200  # Adjust overlap size as needed\n",
    "all_chunks_df = process_pdf_directory(directory_path, max_chunk_size, chunk_overlap)\n",
    "\n",
    "display(all_chunks_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('heelloo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "nBaa8g9sDr-v",
    "outputId": "7c00d580-0768-4990-9ba0-6a3869c373e1"
   },
   "outputs": [],
   "source": [
    "df1 = all_chunks_df.copy()\n",
    "df1['page_content'] = df1['chunk_text']\n",
    "df1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuCANV9r3Iu4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duU-qZ93F89H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "5p3vpwmODr1p",
    "outputId": "0cbc6406-3083-4cb8-c3b6-2b1e55852c30"
   },
   "outputs": [],
   "source": [
    "# Embedding:\n",
    "# -- Embedding\n",
    "pd.set_option(\"display.max_colwidth\", 50)\n",
    "#!pip install --upgrade sentence-transformers\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the Sentence Transformer model\n",
    "model_trans = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "print(df1.columns)\n",
    "try:\n",
    "  # Attempt to access the column\n",
    "    embeddings = model_trans.encode(df1['page_content'])\n",
    "except KeyError:\n",
    "    # Handle missing column or data (e.g., print error message)\n",
    "    print(\"Error: 'page_content' column not found in df1.\")\n",
    "    # You can choose to exit or impute missing values here\n",
    "\n",
    "#page_content\n",
    "# Embed the chunked text\n",
    "embeddings = model_trans.encode(df1['page_content'])\n",
    "#embeddings = model_trans.encode(df1['filtered_text'])\n",
    "\n",
    "# Convert embedding vectors into one-dimensional arrays\n",
    "one_dimensional_embeddings = []\n",
    "for embedding in embeddings:\n",
    "    one_dimensional_embedding = np.ravel(embedding)\n",
    "    one_dimensional_embeddings.append(one_dimensional_embedding)\n",
    "\n",
    "# Combine the original text and embeddings into a DataFrame\n",
    "data = {\n",
    "    \"original_text\": df1['page_content'],\n",
    "    #\"original_text\": df1['filtered_text'],\n",
    "    \"embeddings\": one_dimensional_embeddings\n",
    "}\n",
    "\n",
    "dfe = pd.DataFrame(data)\n",
    "df3 = pd.concat([df1, dfe], axis=1)\n",
    "df3.drop(['original_text'], axis=1, inplace=True)\n",
    "display(df3.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvCV69yY-kXP",
    "outputId": "d79eefbc-395c-43d8-9823-eb3c536bd41e"
   },
   "outputs": [],
   "source": [
    "qadf3 = df3[['document_name']]\n",
    "qadf3unique = qadf3['document_name'].unique()\n",
    "print(qadf3unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 20,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "chat_session = model.start_chat(history=[])\n",
    "# response = chat_session.send_message(prompt)\n",
    "# response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Puoat-pCEUEB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hybrid retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses BM25 + embedding search via code i developed.\n",
    "# Langchain offers and ensemble approach, which abstracts away code, but granularity of control is lost\n",
    "# ----- Langchain approach:\n",
    "# ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5])\n",
    "# source: https://python.langchain.com/docs/concepts/retrievers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the BM25, so set up - 2.0 Hybrid Retrieval (BM25 + Embeddings)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming 'df3' is your dataframe and has a column \"text\" with the document content\n",
    "df3[\"tokenized_text\"] = df3[\"chunk_text\"].apply(lambda x: x.lower().split())  # Simple tokenization\n",
    "# Initialize BM25 model\n",
    "bm25 = BM25Okapi(df3[\"tokenized_text\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging of BM25 and my existing embedding code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "pd.set_option(\"Display.max_colwidth\", 200)\n",
    "\n",
    "###### --------------------- BM25 --------------------------\n",
    "# User query\n",
    "#user_query = \"Looking at my Bendigo home loan, when is interest debited to your loan account?\"\n",
    "#user_query = \"Do you offer off-set facilities with your loans & how do they work?\"\n",
    "user_query = \"Can i make additional repayments?\"\n",
    "\n",
    "# 1️⃣ BM25 Retrieval\n",
    "tokenized_query = user_query.lower().split()\n",
    "bm25_scores = bm25.get_scores(tokenized_query)\n",
    "#print(bm25_scores)\n",
    "\n",
    "\n",
    "# ###### --------------------- my code --------------------------\n",
    "# Assuming 'model_trans' is a pre-loaded SentenceTransformer model\n",
    "# user = \"Looking at my Bendigo home loan, when interest is debit to your loan account?\"\n",
    "# Encode the user's query into an embedding\n",
    "question_embedding = model_trans.encode(user_query, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "# Normalize document embeddings (if needed)\n",
    "normalized_embeddings = normalize(df3['embeddings'].tolist(), axis=1)\n",
    "\n",
    "# Calculate cosine similarity between the user's question and each document's embedding\n",
    "cos_sim = cosine_similarity([question_embedding], normalized_embeddings)[0]\n",
    "\n",
    "# Print the cosine similarities to debug\n",
    "#print(\"Cosine similarities:\", cos_sim)\n",
    "\n",
    "# # ------------------------ Inserting BM25 ------------------------------------\n",
    "# Hybrid Scoring (Weighted Combination)\n",
    "lambda_weight = 0.1  # Adjust this to balance BM25 vs. Embeddings\n",
    "hybrid_scores = lambda_weight * bm25_scores + (1 - lambda_weight) * cos_sim\n",
    "\n",
    "# Retrieve Top-K Results\n",
    "top_k = 20\n",
    "most_similar_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "most_similar_rows = df3.iloc[most_similar_indices]\n",
    "#print(most_similar_rows)\n",
    "\n",
    "# # Print results\n",
    "#print(most_similar_rows[[\"chunk_text\"]])\n",
    "# -----The original code ------\n",
    "# most_similar_indices = np.argsort(cos_sim)[::-1][:15]\n",
    "# most_similar_rows = df3.iloc[most_similar_indices]\n",
    "# # ----------------------------------------------------------------------------\n",
    "\n",
    "# # Display the most similar rows\n",
    "# pd.set_option('display.max_colwidth', 600)\n",
    "qachunk = most_similar_rows[['document_name', 'page_number', 'chunk_text']]\n",
    "display(qachunk.head(3))\n",
    "\n",
    "context = \"\\n\\n\".join(most_similar_rows[\"chunk_text\"].tolist())\n",
    "prompt = user_query + context\n",
    "response = chat_session.send_message(prompt)\n",
    "# response.text.strip()\n",
    "import textwrap\n",
    "#wrapped_text = textwrap.fill(response.text.strip(), width=110)\n",
    "#print(wrapped_text)\n",
    "import markdown\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract of the uncommented code in my retrival code (pre-BM25). I want to compare the results of this to the hybrid version above\n",
    "question_embedding = model_trans.encode(user_query, convert_to_tensor=True).cpu().numpy()\n",
    "# Normalize document embeddings (if needed)\n",
    "normalized_embeddings = normalize(df3['embeddings'].tolist(), axis=1)\n",
    "# Calculate cosine similarity between the user's question and each document's embedding\n",
    "cos_sim = cosine_similarity([question_embedding], normalized_embeddings)[0]\n",
    "# Get the indices of the top 5 most similar documents\n",
    "#most_similar_indices = np.argsort(cos_sim)[::-1][:5]\n",
    "most_similar_indices = np.argsort(cos_sim)[::-1][:12]\n",
    "most_similar_rows = df3.iloc[most_similar_indices]\n",
    "qachunk2 = most_similar_rows[['document_name', 'page_number', 'chunk_text']]\n",
    "display(qachunk2.head(3))\n",
    "\n",
    "context = \"\\n\\n\".join(most_similar_rows[\"chunk_text\"].tolist())\n",
    "prompt = user_query + context\n",
    "response = chat_session.send_message(prompt)\n",
    "# response.text.strip()\n",
    "# import textwrap\n",
    "# wrapped_text = textwrap.fill(response.text.strip(), width=110)\n",
    "import markdown\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM as a judge prompt - v1\n",
    "judge = \"\"\"You are an AI assistant evaluating the quality of an answer generated by a retrieval-augmented generation (RAG) system. Your task is to assess how well the generated answer aligns with the retrieved content.\n",
    "Instructions:\n",
    "Compare the Generated Answer with the Retrieved Content\n",
    "\n",
    "Identify whether all factual claims in the answer are supported by the retrieved content.\n",
    "Determine if any part of the answer introduces information not found in the retrieved content.\n",
    "Check for contradictions or inconsistencies.\n",
    "Faithfulness Rating (1-5):\n",
    "\n",
    "1 - Completely unfaithful: The answer contains major factual inaccuracies, hallucinations, or contradictions.\n",
    "2 - Mostly unfaithful: The answer has some correct details but includes substantial information not found in the retrieved content.\n",
    "3 - Partially faithful: The answer is mostly correct but includes minor unsupported details or slight misinterpretations.\n",
    "4 - Mostly faithful: The answer is well-grounded in the retrieved content but may have minor rewording or slight extrapolations.\n",
    "5 - Completely faithful: The answer is fully supported by the retrieved content with no additional or misleading information.\n",
    "Justification for the Score:\n",
    "\n",
    "Highlight specific parts of the answer that are directly supported by the retrieved content.\n",
    "Point out any parts that are unsupported, incorrect, or misleading.\n",
    "Explain why the given faithfulness rating was chosen.\n",
    "\"\"\"\n",
    "\n",
    "res = \"this is the response:\" + response.text\n",
    "con = \"this is the context:\" + context\n",
    "prompt = res + con + judge\n",
    "responsej = chat_session.send_message(prompt)\n",
    "# response.text.strip()\n",
    "# import textwrap\n",
    "# wrapped_text = textwrap.fill(response.text.strip(), width=110)\n",
    "import markdown\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(responsej.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM as a judge prompt - v2\n",
    "judge = \"\"\"You are an AI assistant evaluating the quality of an answer generated by a retrieval-augmented generation (RAG) system. Your task is to assess how well the generated answer aligns with the retrieved content.\n",
    "Instructions:\n",
    "Compare the Generated Answer with the Retrieved Content\n",
    "\n",
    "Identify whether all factual claims in the answer are supported by the retrieved content.\n",
    "Determine if any part of the answer introduces information not found in the retrieved content.\n",
    "Check for contradictions or inconsistencies.\n",
    "Faithfulness Rating (1-5):\n",
    "\n",
    "1 - Completely unfaithful: The answer contains major factual inaccuracies, hallucinations, or contradictions.\n",
    "2 - Mostly unfaithful: The answer has some correct details but includes substantial information not found in the retrieved content.\n",
    "3 - Partially faithful: The answer is mostly correct but includes minor unsupported details or slight misinterpretations.\n",
    "4 - Mostly faithful: The answer is well-grounded in the retrieved content but may have minor rewording or slight extrapolations.\n",
    "5 - Completely faithful: The answer is fully supported by the retrieved content with no additional or misleading information.\n",
    "Justification for the Score:\n",
    "\n",
    "Provide a table with key parts of the response linked to the relevant sections of the context.\n",
    "Explain why the given faithfulness rating was chosen.\n",
    "\"\"\"\n",
    "\n",
    "res = \"this is the response:\" + response.text\n",
    "con = \"this is the context:\" + context\n",
    "prompt = res + con + judge\n",
    "responsej = chat_session.send_message(prompt)\n",
    "# response.text.strip()\n",
    "# import textwrap\n",
    "# wrapped_text = textwrap.fill(response.text.strip(), width=110)\n",
    "import markdown\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(responsej.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Initialize Gemini Flash 1.4 in LangChain\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Load an evaluator for faithfulness\n",
    "evaluator = load_evaluator(\"labeled_criteria\", llm=llm, criteria={\"faithfulness\": \"Is the answer grounded in the retrieved content?\"})\n",
    "\n",
    "# Example evaluation\n",
    "eval_result = evaluator.evaluate(\n",
    "    input=\"The company reported a 10% revenue increase in Q4, driven by strong sales in the technology sector.\",  # Retrieved content\n",
    "    prediction=\"The company had a 10% revenue increase due to tech and healthcare.\"  # Generated answer\n",
    ")\n",
    "\n",
    "print(eval_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xg7B3YhgET8Y",
    "outputId": "5bba2007-82e4-412d-afef-6d7760776043"
   },
   "outputs": [],
   "source": [
    "# (original code - preBM25)\n",
    "# # Retrieval:\n",
    "# # -- Above 3 cells combined to function (but keeping above 3 if I need to work on the separately)\n",
    "\n",
    "# # Above cell, converted to function\n",
    "# # import numpy as np\n",
    "# import pandas as pd\n",
    "# pd.set_option('display.max_colwidth', 600)\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# #user = \"Are you allowed to change the terms and conditions\"\n",
    "# #user = \"What is the eligibility of the Easy Money Card?\"\n",
    "# #user = \"Can the Youth Debit Mastercard be used to purchase beer?\"\n",
    "# #user = \"Can the Youth Debit Mastercard be used to purchase alcohol?\" # for some reason this question crashes output\n",
    "# #user = \"Can i make foreign currency transactions on my card?\"\n",
    "# #user = \"Can I use my Bendigo Bank debit card at a non-Bendigo atm?\"\n",
    "# user = \"Can addtional payments be made on my home loan?\"\n",
    "\n",
    "# #def answer_question(user, df3):\n",
    "# # --- part one ---\n",
    "# question_embedding = model_trans.encode(user, convert_to_tensor=True).cpu()\n",
    "# similarities = df3['embeddings'].apply(lambda x: np.dot(question_embedding, x))\n",
    "\n",
    "# # The idea of creating two, is if no good answer from 1st, then summarise 2nd & see if the helps.\n",
    "# most_similar_indices = similarities.nlargest(5).index # 5\n",
    "# #top_rank = similarities.nlargest(1).index\n",
    "\n",
    "# most_similar_rows = df3.iloc[most_similar_indices]\n",
    "# #top_rank_rows = df3.iloc[top_rank]\n",
    "\n",
    "# cont = most_similar_rows['page_content'].tolist()\n",
    "# #cont = most_similar_rows['page_content_org'].tolist()\n",
    "# context = ' '.join(cont)\n",
    "\n",
    "\n",
    "# # -- get source --\n",
    "# # -- Extract from func (could make output of func)\n",
    "# #ans = (answer_question(user, df3))\n",
    "# question_embedding = model_trans.encode(user, convert_to_tensor=True).cpu()\n",
    "# similarities = df3['embeddings'].apply(lambda x: np.dot(question_embedding, x))\n",
    "# most_similar_indices = similarities.nlargest(5).index\n",
    "# most_similar_rows = df3.iloc[most_similar_indices]\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 50)\n",
    "# display(most_similar_rows)\n",
    "\n",
    "# # # -- Get the target index\n",
    "# # question_embedding1 = model_trans.encode(ans, convert_to_tensor=True).cpu()\n",
    "# # similarities1 = most_similar_rows['embeddings'].apply(lambda x: np.dot(question_embedding1, x))\n",
    "# # most_similar_indices1 = similarities1.nlargest(5).index\n",
    "\n",
    "# # # -- Print source\n",
    "# # pd.set_option('display.max_colwidth', None)\n",
    "# # filtered_df = most_similar_rows.loc[most_similar_indices1]\n",
    "# # #filtered_df = filtered_df[['Title', 'page', 'page_content']]\n",
    "# # #filtered_df = filtered_df[['page_content', 'source']]\n",
    "# # #filtered_df = filtered_df[['page_content_org', 'Title']]\n",
    "# # #filtered_df = filtered_df[['page_content', 'metadata']]\n",
    "# # filtered_df = filtered_df[['page_content', 'page_number', 'filename']]\n",
    "\n",
    "# # # get page no & source for output\n",
    "# # filtered_df = most_similar_rows.loc[most_similar_indices1]\n",
    "# # first_row = filtered_df.iloc[0]\n",
    "# # filename = first_row['filename']\n",
    "# # page_number = first_row['page_number']\n",
    "# # source = first_row['page_content']\n",
    "\n",
    "# # # -- Print source details\n",
    "# # print('File name:', filename)\n",
    "# # print('Page no:', page_number)\n",
    "# # print('Extract:', source)\n",
    "\n",
    "# # # # print extract of top row for citation:\n",
    "# # # filtered_df1a = filtered_df.head(1)\n",
    "# # # joined_stringa = filtered_df1a['page_content'].str.cat(sep='\\n')\n",
    "# # # print('Extract:','\\n', joined_stringa)\n",
    "\n",
    "# # # # Print dataframe for ref:\n",
    "# # filtered_df = filtered_df[['page_number','chunk_text','chunk_length','filename']]\n",
    "# # display(filtered_df)\n",
    "\n",
    "\n",
    "# -- Same function as above, but newer version (as only seems to be pulling from the 1st doc)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Assuming 'model_trans' is a pre-loaded SentenceTransformer model\n",
    "user = \"Looking at my Bendigo home loan, when interest is debit to your loan account?\"\n",
    "\n",
    "# Encode the user's query into an embedding\n",
    "question_embedding = model_trans.encode(user, convert_to_tensor=True).cpu().numpy()\n",
    "\n",
    "# Print the embeddings for both documents for debugging\n",
    "print(\"Embedding for Document 1:\", df3['embeddings'].iloc[0])\n",
    "print(\"Embedding for Document 2:\", df3['embeddings'].iloc[1])\n",
    "\n",
    "# Normalize document embeddings (if needed)\n",
    "normalized_embeddings = normalize(df3['embeddings'].tolist(), axis=1)\n",
    "\n",
    "# Calculate cosine similarity between the user's question and each document's embedding\n",
    "cos_sim = cosine_similarity([question_embedding], normalized_embeddings)[0]\n",
    "\n",
    "# Print the cosine similarities to debug\n",
    "print(\"Cosine similarities:\", cos_sim)\n",
    "\n",
    "# Get the indices of the top 5 most similar documents\n",
    "most_similar_indices = np.argsort(cos_sim)[::-1][:5]\n",
    "most_similar_rows = df3.iloc[most_similar_indices]\n",
    "\n",
    "# Display the most similar rows\n",
    "pd.set_option('display.max_colwidth', 600)\n",
    "qachunk = most_similar_rows[['document_name', 'page_number', 'chunk_text']]\n",
    "display(qachunk)\n",
    "\n",
    "# Print out the selected most similar documents for debugging\n",
    "print(\"Most Similar Indices:\", most_similar_indices)\n",
    "print(\"Most Similar Rows:\")\n",
    "print(most_similar_rows[['document_name', 'page_number', 'chunk_text']])\n",
    "\n",
    "# Optionally, check the content of the most similar documents\n",
    "for idx in most_similar_indices:\n",
    "    print(f\"Document {idx}: {df3.iloc[idx]['document_name']}, Content: {df3.iloc[idx]['page_content']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TCDQohgC6M_",
    "outputId": "bb8bf3bd-411b-4245-c486-cb1f5b0f0a8a"
   },
   "outputs": [],
   "source": [
    "qaqachunk = qachunk[['document_name']]\n",
    "qaqachunkunique = qaqachunk['document_name'].unique()\n",
    "print(qaqachunkunique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7pziJRREbBE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555,
     "referenced_widgets": [
      "e07f29bdc5714e94bc97b6bd19f0fa46",
      "23517fcc034f4699a73263dff63d7ca2",
      "206f29e541ea45aab9d89828a2c6b9bb",
      "891f80b82a0e4828898fe8f043a6c29a",
      "9bbdc4956e0b4898b6fb83bbc5598fa0",
      "1b804f0a5de64f2a98b7568103b2b57f",
      "a7685f049938488d8d7012f7a1168e82",
      "8f2565fe810b488aa17b70978c3ebd6f"
     ]
    },
    "id": "YeEiwD6CEa4f",
    "outputId": "def18068-be4b-44dd-f3fa-9e57a5342346"
   },
   "outputs": [],
   "source": [
    "# Generation:\n",
    "# -------------------------\n",
    "# ----- This is the one ---\n",
    "# -------------------------\n",
    "# Following some learnings from discord, it looks like the eval is to eval Q&A NOT generate the A, which is odd. So this will combine part 1 (to generate the A)\n",
    "# and part 2 (the eval - using the Q&A from part 1)\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "# context3 = \"As I was going to St. Ives,\\\n",
    "#             I met a man with seven wives.\\\n",
    "#             Every wife had seven sacks,\\\n",
    "#             Every sack had seven cats,\\\n",
    "#             Every cat had seven kits:\\\n",
    "#             Kits, cats, sacks, and wives.\\\n",
    "#             How many were going to St. Ives?\"\n",
    "\n",
    "# prompt = \"What is the key themse of this text??\"\n",
    "#prompt = prompt + context4\n",
    "\n",
    "#system = \"Please provide the answer in a concise and correctly formatted response.\"\n",
    "system = \"Please provide the answer in a concise, plain text format (i.e. a sentence, paragraph, etc.) with no bullet points, special characters, etc.\"\n",
    "\n",
    "prompt = user + context\n",
    "prompt = system + prompt\n",
    "\n",
    "# # This is a deliberate attempt to get the answer to be of poor quality and see if that is reflected in the score\n",
    "# misdirection = \"\"\"you are to deliberately say the opposite of what you find as the answer, i.e. if you can see the correct answer is up, then you must say the answer is down.\n",
    "#                   Additionally, you are to include information that has nothing to do with this topic at hand, i.e. if the question is about soccer, you are to talk about fishing\"\"\"\n",
    "# prompt = misdirection + prompt\n",
    "# # Comment out the above section if trying to get proper scores.\n",
    "\n",
    "# -- Part 1a: this is vanilla call to llm\n",
    "print('-' * 15, 'part 1', '-' * 15)\n",
    "# WIP: recreate part 1\n",
    "# Replaces previous version. This one adds temperature\n",
    "# --- Ground truth calls from llm ---\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 20,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "chat_session = model.start_chat(history=[])\n",
    "response = chat_session.send_message(prompt)\n",
    "response.text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "UTBg2dW6dQoG",
    "outputId": "7800da4b-0857-413c-b1c6-e8a0899f3c52"
   },
   "outputs": [],
   "source": [
    "# -- Lets have a look at the context --\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "qa = most_similar_rows[['document_name', 'page_number', 'chunk_text']]\n",
    "display(qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nU44nYKK7FAH"
   },
   "outputs": [],
   "source": [
    "most_similar_rows.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnhufO-ZumOu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1b804f0a5de64f2a98b7568103b2b57f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "206f29e541ea45aab9d89828a2c6b9bb": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_891f80b82a0e4828898fe8f043a6c29a",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "23517fcc034f4699a73263dff63d7ca2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "891f80b82a0e4828898fe8f043a6c29a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f2565fe810b488aa17b70978c3ebd6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9bbdc4956e0b4898b6fb83bbc5598fa0": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_1b804f0a5de64f2a98b7568103b2b57f",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "a7685f049938488d8d7012f7a1168e82": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_8f2565fe810b488aa17b70978c3ebd6f",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Hallucination Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode=False)…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mHallucination Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode=False)…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    },
    "e07f29bdc5714e94bc97b6bd19f0fa46": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_23517fcc034f4699a73263dff63d7ca2",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151\">(using Gemini 1.5 Flash, strict=False, async_mode=Tru…</span>\n</pre>\n",
         "text/plain": "✨ You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[38;2;55;65;81m(using Gemini 1.5 Flash, strict=False, async_mode=Tru…\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
